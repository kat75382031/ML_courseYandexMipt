{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming Assignment: \n",
    "## Готовим LDA по рецептам"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как вы уже знаете, в тематическом моделировании делается предположение о том, что для определения тематики порядок слов в документе не важен; об этом гласит гипотеза «мешка слов». Сегодня мы будем работать с несколько нестандартной для тематического моделирования коллекцией, которую можно назвать «мешком ингредиентов», потому что на состоит из рецептов блюд разных кухонь. Тематические модели ищут слова, которые часто вместе встречаются в документах, и составляют из них темы. Мы попробуем применить эту идею к рецептам и найти кулинарные «темы». Эта коллекция хороша тем, что не требует предобработки. Кроме того, эта задача достаточно наглядно иллюстрирует принцип работы тематических моделей.\n",
    "\n",
    "Для выполнения заданий, помимо часто используемых в курсе библиотек, потребуются модули *json* и *gensim*. Первый входит в дистрибутив Anaconda, второй можно поставить командой \n",
    "\n",
    "*pip install gensim*\n",
    "\n",
    "Построение модели занимает некоторое время. На ноутбуке с процессором Intel Core i7 и тактовой частотой 2400 МГц на построение одной модели уходит менее 10 минут."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загрузка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Коллекция дана в json-формате: для каждого рецепта известны его id, кухня (cuisine) и список ингредиентов, в него входящих. Загрузить данные можно с помощью модуля json (он входит в дистрибутив Anaconda):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "#!pip install gensim==2.3.0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"recipes.json\") as f:\n",
    "    recipes = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 10259, 'cuisine': 'greek', 'ingredients': ['romaine lettuce', 'black olives', 'grape tomatoes', 'garlic', 'pepper', 'purple onion', 'seasoning', 'garbanzo beans', 'feta cheese crumbles']}\n"
     ]
    }
   ],
   "source": [
    "print(recipes[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Составление корпуса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "import gensim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наша коллекция небольшая, и целиком помещается в оперативную память. Gensim может работать с такими данными и не требует их сохранения на диск в специальном формате. Для этого коллекция должна быть представлена в виде списка списков, каждый внутренний список соответствует отдельному документу и состоит из его слов. Пример коллекции из двух документов: \n",
    "\n",
    "[[\"hello\", \"world\"], [\"programming\", \"in\", \"python\"]]\n",
    "\n",
    "Преобразуем наши данные в такой формат, а затем создадим объекты corpus и dictionary, с которыми будет работать модель."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [recipe[\"ingredients\"] for recipe in recipes]\n",
    "dictionary = corpora.Dictionary(texts)   # составляем словарь\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]  # составляем корпус документов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['romaine lettuce', 'black olives', 'grape tomatoes', 'garlic', 'pepper', 'purple onion', 'seasoning', 'garbanzo beans', 'feta cheese crumbles']\n",
      "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1)]\n"
     ]
    }
   ],
   "source": [
    "print(texts[0])\n",
    "print(corpus[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У объекта dictionary есть полезная переменная dictionary.token2id, позволяющая находить соответствие между ингредиентами и их индексами."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение модели\n",
    "Вам может понадобиться [документация](https://radimrehurek.com/gensim/models/ldamodel.html) LDA в gensim."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Задание 1.__ Обучите модель LDA с 40 темами, установив количество проходов по коллекции 5 и оставив остальные параметры по умолчанию. \n",
    "\n",
    "\n",
    "Затем вызовите метод модели *show_topics*, указав количество тем 40 и количество токенов 10, и сохраните результат (топы ингредиентов в темах) в отдельную переменную. Если при вызове метода *show_topics* указать параметр *formatted=True*, то топы ингредиентов будет удобно выводить на печать, если *formatted=False*, будет удобно работать со списком программно. Выведите топы на печать, рассмотрите темы, а затем ответьте на вопрос:\n",
    "\n",
    "Сколько раз ингредиенты \"salt\", \"sugar\", \"water\", \"mushrooms\", \"chicken\", \"eggs\" встретились среди топов-10 всех 40 тем? При ответе __не нужно__ учитывать составные ингредиенты, например, \"hot water\".\n",
    "\n",
    "Передайте 6 чисел в функцию save_answers1 и загрузите сгенерированный файл в форму.\n",
    "\n",
    "У gensim нет возможности фиксировать случайное приближение через параметры метода, но библиотека использует numpy для инициализации матриц. Поэтому, по утверждению автора библиотеки, фиксировать случайное приближение нужно командой, которая написана в следующей ячейке. __Перед строкой кода с построением модели обязательно вставляйте указанную строку фиксации random.seed.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 26s, sys: 500 ms, total: 2min 27s\n",
      "Wall time: 2min 3s\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(76543)\n",
    "# здесь код для построения модели:\n",
    "%time ldamodel = models.ldamodel.LdaModel(corpus, num_topics=40,passes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.052*\"212\" + 0.036*\"362\" + 0.035*\"26\" + 0.033*\"228\" + 0.033*\"105\" + 0.033*\"669\" + 0.032*\"161\" + 0.032*\"629\" + 0.031*\"19\" + 0.028*\"89\"'),\n",
       " (1,\n",
       "  '0.094*\"204\" + 0.077*\"47\" + 0.069*\"11\" + 0.065*\"115\" + 0.050*\"50\" + 0.043*\"312\" + 0.036*\"27\" + 0.027*\"365\" + 0.026*\"531\" + 0.024*\"52\"'),\n",
       " (2,\n",
       "  '0.086*\"146\" + 0.079*\"475\" + 0.073*\"555\" + 0.064*\"48\" + 0.056*\"309\" + 0.042*\"124\" + 0.030*\"781\" + 0.030*\"652\" + 0.025*\"200\" + 0.023*\"613\"'),\n",
       " (3,\n",
       "  '0.079*\"11\" + 0.067*\"40\" + 0.053*\"35\" + 0.050*\"22\" + 0.038*\"43\" + 0.036*\"38\" + 0.034*\"428\" + 0.033*\"12\" + 0.031*\"351\" + 0.029*\"560\"'),\n",
       " (4,\n",
       "  '0.075*\"213\" + 0.065*\"35\" + 0.060*\"250\" + 0.057*\"200\" + 0.043*\"401\" + 0.041*\"11\" + 0.039*\"3\" + 0.038*\"181\" + 0.031*\"13\" + 0.027*\"29\"'),\n",
       " (5,\n",
       "  '0.120*\"78\" + 0.083*\"103\" + 0.053*\"58\" + 0.052*\"105\" + 0.039*\"13\" + 0.037*\"171\" + 0.036*\"32\" + 0.034*\"11\" + 0.031*\"376\" + 0.028*\"79\"'),\n",
       " (6,\n",
       "  '0.084*\"699\" + 0.039*\"953\" + 0.038*\"1413\" + 0.029*\"1001\" + 0.027*\"484\" + 0.026*\"573\" + 0.022*\"770\" + 0.021*\"942\" + 0.021*\"200\" + 0.021*\"1178\"'),\n",
       " (7,\n",
       "  '0.106*\"255\" + 0.073*\"996\" + 0.068*\"185\" + 0.063*\"69\" + 0.039*\"668\" + 0.038*\"1856\" + 0.037*\"483\" + 0.036*\"1085\" + 0.034*\"982\" + 0.031*\"1397\"'),\n",
       " (8,\n",
       "  '0.087*\"60\" + 0.071*\"466\" + 0.049*\"205\" + 0.048*\"29\" + 0.040*\"1082\" + 0.037*\"330\" + 0.031*\"1089\" + 0.028*\"886\" + 0.024*\"1265\" + 0.024*\"47\"'),\n",
       " (9,\n",
       "  '0.091*\"494\" + 0.063*\"324\" + 0.045*\"311\" + 0.044*\"459\" + 0.042*\"301\" + 0.040*\"102\" + 0.036*\"47\" + 0.036*\"248\" + 0.035*\"119\" + 0.035*\"2029\"'),\n",
       " (10,\n",
       "  '0.078*\"252\" + 0.049*\"327\" + 0.046*\"105\" + 0.036*\"208\" + 0.034*\"54\" + 0.033*\"78\" + 0.031*\"10\" + 0.030*\"35\" + 0.027*\"335\" + 0.025*\"11\"'),\n",
       " (11,\n",
       "  '0.091*\"54\" + 0.074*\"190\" + 0.061*\"308\" + 0.050*\"27\" + 0.046*\"11\" + 0.039*\"13\" + 0.037*\"209\" + 0.035*\"3\" + 0.024*\"35\" + 0.024*\"315\"'),\n",
       " (12,\n",
       "  '0.097*\"393\" + 0.080*\"522\" + 0.054*\"988\" + 0.051*\"938\" + 0.045*\"485\" + 0.045*\"803\" + 0.042*\"566\" + 0.041*\"333\" + 0.028*\"774\" + 0.027*\"35\"'),\n",
       " (13,\n",
       "  '0.076*\"35\" + 0.065*\"194\" + 0.049*\"139\" + 0.036*\"11\" + 0.029*\"54\" + 0.029*\"551\" + 0.026*\"105\" + 0.026*\"29\" + 0.026*\"3\" + 0.026*\"140\"'),\n",
       " (14,\n",
       "  '0.119*\"108\" + 0.068*\"465\" + 0.053*\"162\" + 0.053*\"796\" + 0.052*\"876\" + 0.040*\"3\" + 0.037*\"473\" + 0.031*\"54\" + 0.030*\"1343\" + 0.028*\"269\"'),\n",
       " (15,\n",
       "  '0.185*\"191\" + 0.093*\"312\" + 0.060*\"348\" + 0.051*\"991\" + 0.039*\"79\" + 0.037*\"27\" + 0.033*\"949\" + 0.031*\"856\" + 0.030*\"724\" + 0.026*\"460\"'),\n",
       " (16,\n",
       "  '0.065*\"57\" + 0.048*\"11\" + 0.047*\"75\" + 0.045*\"41\" + 0.044*\"111\" + 0.036*\"519\" + 0.030*\"54\" + 0.030*\"38\" + 0.029*\"3\" + 0.028*\"71\"'),\n",
       " (17,\n",
       "  '0.067*\"357\" + 0.056*\"74\" + 0.046*\"221\" + 0.045*\"112\" + 0.042*\"56\" + 0.038*\"3\" + 0.036*\"839\" + 0.031*\"242\" + 0.029*\"731\" + 0.028*\"337\"'),\n",
       " (18,\n",
       "  '0.152*\"184\" + 0.047*\"215\" + 0.046*\"316\" + 0.037*\"27\" + 0.036*\"11\" + 0.033*\"177\" + 0.031*\"875\" + 0.029*\"15\" + 0.027*\"236\" + 0.025*\"278\"'),\n",
       " (19,\n",
       "  '0.076*\"649\" + 0.069*\"431\" + 0.057*\"434\" + 0.048*\"253\" + 0.046*\"29\" + 0.046*\"129\" + 0.038*\"345\" + 0.036*\"621\" + 0.029*\"46\" + 0.026*\"243\"'),\n",
       " (20,\n",
       "  '0.101*\"277\" + 0.084*\"42\" + 0.076*\"247\" + 0.055*\"514\" + 0.052*\"72\" + 0.045*\"2028\" + 0.043*\"415\" + 0.040*\"869\" + 0.039*\"97\" + 0.029*\"496\"'),\n",
       " (21,\n",
       "  '0.112*\"115\" + 0.110*\"11\" + 0.107*\"15\" + 0.098*\"18\" + 0.073*\"27\" + 0.059*\"53\" + 0.051*\"65\" + 0.041*\"47\" + 0.038*\"290\" + 0.034*\"310\"'),\n",
       " (22,\n",
       "  '0.066*\"187\" + 0.063*\"706\" + 0.041*\"651\" + 0.041*\"1031\" + 0.039*\"464\" + 0.034*\"521\" + 0.031*\"83\" + 0.031*\"380\" + 0.030*\"608\" + 0.026*\"164\"'),\n",
       " (23,\n",
       "  '0.154*\"229\" + 0.053*\"899\" + 0.052*\"1172\" + 0.038*\"794\" + 0.034*\"1454\" + 0.028*\"321\" + 0.026*\"157\" + 0.025*\"33\" + 0.023*\"470\" + 0.021*\"1605\"'),\n",
       " (24,\n",
       "  '0.058*\"41\" + 0.051*\"11\" + 0.048*\"35\" + 0.044*\"109\" + 0.035*\"347\" + 0.033*\"3\" + 0.032*\"19\" + 0.029*\"123\" + 0.029*\"29\" + 0.027*\"90\"'),\n",
       " (25,\n",
       "  '0.061*\"597\" + 0.055*\"439\" + 0.054*\"47\" + 0.045*\"132\" + 0.043*\"312\" + 0.038*\"320\" + 0.037*\"11\" + 0.036*\"579\" + 0.036*\"50\" + 0.035*\"115\"'),\n",
       " (26,\n",
       "  '0.132*\"49\" + 0.067*\"372\" + 0.066*\"387\" + 0.055*\"1044\" + 0.052*\"541\" + 0.045*\"1108\" + 0.042*\"821\" + 0.038*\"389\" + 0.027*\"1212\" + 0.024*\"1077\"'),\n",
       " (27,\n",
       "  '0.075*\"437\" + 0.060*\"188\" + 0.036*\"804\" + 0.035*\"273\" + 0.035*\"1099\" + 0.032*\"1349\" + 0.030*\"742\" + 0.027*\"1173\" + 0.027*\"295\" + 0.023*\"298\"'),\n",
       " (28,\n",
       "  '0.065*\"221\" + 0.062*\"145\" + 0.043*\"47\" + 0.030*\"32\" + 0.027*\"864\" + 0.023*\"29\" + 0.022*\"560\" + 0.022*\"19\" + 0.021*\"222\" + 0.021*\"105\"'),\n",
       " (29,\n",
       "  '0.082*\"11\" + 0.080*\"122\" + 0.079*\"105\" + 0.071*\"280\" + 0.055*\"54\" + 0.035*\"31\" + 0.035*\"13\" + 0.033*\"214\" + 0.032*\"29\" + 0.032*\"116\"'),\n",
       " (30,\n",
       "  '0.103*\"272\" + 0.070*\"245\" + 0.061*\"201\" + 0.053*\"577\" + 0.042*\"323\" + 0.037*\"228\" + 0.037*\"244\" + 0.030*\"696\" + 0.027*\"193\" + 0.025*\"728\"'),\n",
       " (31,\n",
       "  '0.190*\"24\" + 0.101*\"34\" + 0.071*\"216\" + 0.038*\"772\" + 0.035*\"13\" + 0.034*\"31\" + 0.029*\"38\" + 0.027*\"82\" + 0.025*\"11\" + 0.025*\"79\"'),\n",
       " (32,\n",
       "  '0.095*\"397\" + 0.063*\"863\" + 0.063*\"47\" + 0.053*\"524\" + 0.037*\"29\" + 0.035*\"11\" + 0.032*\"1218\" + 0.026*\"444\" + 0.025*\"9\" + 0.021*\"1339\"'),\n",
       " (33,\n",
       "  '0.216*\"4\" + 0.149*\"11\" + 0.086*\"35\" + 0.056*\"3\" + 0.042*\"29\" + 0.040*\"207\" + 0.029*\"54\" + 0.022*\"730\" + 0.019*\"12\" + 0.019*\"154\"'),\n",
       " (34,\n",
       "  '0.149*\"313\" + 0.098*\"37\" + 0.056*\"235\" + 0.053*\"20\" + 0.041*\"916\" + 0.033*\"206\" + 0.030*\"835\" + 0.024*\"13\" + 0.019*\"1291\" + 0.018*\"54\"'),\n",
       " (35,\n",
       "  '0.096*\"26\" + 0.052*\"94\" + 0.047*\"3\" + 0.047*\"99\" + 0.041*\"47\" + 0.033*\"228\" + 0.032*\"95\" + 0.032*\"29\" + 0.030*\"351\" + 0.028*\"11\"'),\n",
       " (36,\n",
       "  '0.086*\"233\" + 0.083*\"1049\" + 0.066*\"1276\" + 0.055*\"1874\" + 0.052*\"737\" + 0.035*\"530\" + 0.031*\"1030\" + 0.021*\"1559\" + 0.019*\"976\" + 0.019*\"68\"'),\n",
       " (37,\n",
       "  '0.079*\"5\" + 0.056*\"54\" + 0.050*\"78\" + 0.047*\"11\" + 0.046*\"489\" + 0.044*\"110\" + 0.040*\"105\" + 0.037*\"12\" + 0.027*\"77\" + 0.027*\"535\"'),\n",
       " (38,\n",
       "  '0.059*\"54\" + 0.044*\"11\" + 0.042*\"82\" + 0.041*\"190\" + 0.036*\"3\" + 0.033*\"230\" + 0.030*\"391\" + 0.028*\"261\" + 0.025*\"477\" + 0.022*\"77\"'),\n",
       " (39,\n",
       "  '0.108*\"658\" + 0.062*\"1125\" + 0.056*\"656\" + 0.051*\"480\" + 0.043*\"664\" + 0.041*\"1396\" + 0.035*\"893\" + 0.033*\"7\" + 0.031*\"1137\" + 0.030*\"726\"')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics=ldamodel.show_topics(num_topics=40,num_words=10)\n",
    "topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19.  8. 10.  0.  1.  2.]\n"
     ]
    }
   ],
   "source": [
    "dictionary.token2id['sugar']\n",
    "count=np.zeros(6)\n",
    "selected_ingredients=[\"salt\", \"sugar\", \"water\", \"mushrooms\", \"chicken\", \"eggs\"]\n",
    "ingredient_id=[]\n",
    "for i in range(len(selected_ingredients)):\n",
    "    ingredient_id.append('\"' + str(dictionary.token2id[selected_ingredients[i]])+'\"')\n",
    "\n",
    "for i in range(len(topics)):\n",
    "    for j in range(len(ingredient_id)):\n",
    "        if (topics[i][1].find(ingredient_id[j])!=-1):\n",
    "            count[j]+=1\n",
    "print(count)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_answers1(c_salt, c_sugar, c_water, c_mushrooms, c_chicken, c_eggs):\n",
    "    with open(\"cooking_LDA_pa_task1.txt\", \"w\") as fout:\n",
    "        fout.write(\" \".join([str(el) for el in [c_salt, c_sugar, c_water, c_mushrooms, c_chicken, c_eggs]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23 9 8 1 0 2"
     ]
    }
   ],
   "source": [
    "save_answers1(23,9,8,1,0,2)\n",
    "!cat cooking_LDA_pa_task1.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Фильтрация словаря\n",
    "В топах тем гораздо чаще встречаются первые три рассмотренных ингредиента, чем последние три. При этом наличие в рецепте курицы, яиц и грибов яснее дает понять, что мы будем готовить, чем наличие соли, сахара и воды. Таким образом, даже в рецептах есть слова, часто встречающиеся в текстах и не несущие смысловой нагрузки, и поэтому их не желательно видеть в темах. Наиболее простой прием борьбы с такими фоновыми элементами — фильтрация словаря по частоте. Обычно словарь фильтруют с двух сторон: убирают очень редкие слова (в целях экономии памяти) и очень частые слова (в целях повышения интерпретируемости тем). Мы уберем только частые слова."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "dictionary2 = copy.deepcopy(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Задание 2.__ У объекта dictionary2 есть переменная *dfs* — это словарь, ключами которого являются id токена, а элементами — число раз, сколько слово встретилось во всей коллекции. Сохраните в отдельный список ингредиенты, которые встретились в коллекции больше 4000 раз. Вызовите метод словаря *filter_tokens*, подав в качестве первого аргумента полученный список популярных ингредиентов. Вычислите две величины: dict_size_before и dict_size_after — размер словаря до и после фильтрации.\n",
    "\n",
    "Затем, используя новый словарь, создайте новый корпус документов, corpus2, по аналогии с тем, как это сделано в начале ноутбука. Вычислите две величины: corpus_size_before и corpus_size_after — суммарное количество ингредиентов в корпусе (для каждого документа вычислите число различных ингредиентов в нем и просуммируйте по всем документам) до и после фильтрации.\n",
    "\n",
    "Передайте величины dict_size_before, dict_size_after, corpus_size_before, corpus_size_after в функцию save_answers2 и загрузите сгенерированный файл в форму."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_frequent=[i for i in np.arange(len(dictionary2.dfs)) if dictionary2.dfs[i]>4000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6714\n",
      "6702\n"
     ]
    }
   ],
   "source": [
    "dict_size_before=len(dictionary2) \n",
    "dictionary2.filter_tokens(most_frequent)\n",
    "dict_size_after=len(dictionary2) \n",
    "print(dict_size_before)\n",
    "print(dict_size_after)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "428249\n",
      "343665\n"
     ]
    }
   ],
   "source": [
    "corpus2=[dictionary2.doc2bow(text) for text in texts]\n",
    "corpus_size_before=0\n",
    "for i in range(len(corpus)):\n",
    "    corpus_size_before+=len(corpus[i])\n",
    "print(corpus_size_before)   \n",
    "\n",
    "corpus_size_after=0\n",
    "for i in range(len(corpus)):\n",
    "    corpus_size_after+=len(corpus2[i])\n",
    "print(corpus_size_after) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_answers2(dict_size_before, dict_size_after, corpus_size_before, corpus_size_after):\n",
    "    with open(\"cooking_LDA_pa_task2.txt\", \"w\") as fout:\n",
    "        fout.write(\" \".join([str(el) for el in [dict_size_before, dict_size_after, corpus_size_before, corpus_size_after]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6714 6702 428249 343665"
     ]
    }
   ],
   "source": [
    "save_answers2(dict_size_before, dict_size_after, corpus_size_before, corpus_size_after)\n",
    "!cat cooking_LDA_pa_task2.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сравнение когерентностей\n",
    "__Задание 3.__ Постройте еще одну модель по корпусу corpus2 и словарю dictionary2, остальные параметры оставьте такими же, как при первом построении модели. Сохраните новую модель в другую переменную (не перезаписывайте предыдущую модель). Не забудьте про фиксирование seed!\n",
    "\n",
    "Затем воспользуйтесь методом *top_topics* модели, чтобы вычислить ее когерентность. Передайте в качестве аргумента соответствующий модели корпус. Метод вернет список кортежей (топ токенов, когерентность), отсортированных по убыванию последней. Вычислите среднюю по всем темам когерентность для каждой из двух моделей и передайте в функцию save_answers3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min, sys: 536 ms, total: 2min 1s\n",
      "Wall time: 1min 36s\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(76543)\n",
    "# здесь код для построения модели:\n",
    "%time ldamodel2 = models.ldamodel.LdaModel(corpus2, num_topics=40,passes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_topics=ldamodel.top_topics(corpus)\n",
    "top_topics2=ldamodel2.top_topics(corpus2)\n",
    "\n",
    "coherence=0\n",
    "coherence2=0\n",
    "for i in range(len(top_topics)):\n",
    "    coherence+=top_topics[i][1]\n",
    "    coherence2+=top_topics2[i][1]\n",
    "coherence/=len(top_topics)\n",
    "coherence2/=len(top_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_answers3(coherence, coherence2):\n",
    "    with open(\"cooking_LDA_pa_task3.txt\", \"w\") as fout:\n",
    "        fout.write(\" \".join([\"%3f\"%el for el in [coherence, coherence2]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-703.201937 -747.695962"
     ]
    }
   ],
   "source": [
    "save_answers3(coherence, coherence2)\n",
    "!cat cooking_LDA_pa_task3.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Считается, что когерентность хорошо соотносится с человеческими оценками интерпретируемости тем. Поэтому на больших текстовых коллекциях когерентность обычно повышается, если убрать фоновую лексику. Однако в нашем случае этого не произошло. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Изучение влияния гиперпараметра alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом разделе мы будем работать со второй моделью, то есть той, которая построена по сокращенному корпусу. \n",
    "\n",
    "Пока что мы посмотрели только на матрицу темы-слова, теперь давайте посмотрим на матрицу темы-документы. Выведите темы для нулевого (или любого другого) документа из корпуса, воспользовавшись методом *get_document_topics* второй модели:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "ldamodel2.minimum_probability=0.01\n",
    "doc_topics2=ldamodel2.get_document_topics(corpus2)\n",
    "print(len(doc_topics2[39773]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также выведите содержимое переменной *.alpha* второй модели:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.025 0.025 0.025 0.025 0.025 0.025 0.025 0.025 0.025 0.025 0.025 0.025\n",
      " 0.025 0.025 0.025 0.025 0.025 0.025 0.025 0.025 0.025 0.025 0.025 0.025\n",
      " 0.025 0.025 0.025 0.025 0.025 0.025 0.025 0.025 0.025 0.025 0.025 0.025\n",
      " 0.025 0.025 0.025 0.025]\n"
     ]
    }
   ],
   "source": [
    "print(ldamodel2.alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У вас должно получиться, что документ характеризуется небольшим числом тем. Попробуем поменять гиперпараметр alpha, задающий априорное распределение Дирихле для распределений тем в документах."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Задание 4.__ Обучите третью модель: используйте сокращенный корпус (corpus2 и dictionary2) и установите параметр __alpha=1__, passes=5. Не забудьте про фиксацию seed! Выведите темы новой модели для нулевого документа; должно получиться, что распределение над множеством тем практически равномерное. Чтобы убедиться в том, что во второй модели документы описываются гораздо более разреженными распределениями, чем в третьей, посчитайте суммарное количество элементов, __превосходящих 0.01__, в матрицах темы-документы обеих моделей. Другими словами, запросите темы  модели для каждого документа с параметром *minimum_probability=0.01* и просуммируйте число элементов в получаемых массивах. Передайте две суммы (сначала для модели с alpha по умолчанию, затем для модели в alpha=1) в функцию save_answers4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 40s, sys: 404 ms, total: 1min 40s\n",
      "Wall time: 1min 15s\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(76543)\n",
    "# здесь код для построения модели:\n",
    "%time ldamodel3 = models.ldamodel.LdaModel(corpus2, num_topics=40,alpha=1,passes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldamodel3.minimum_probability=0.01\n",
    "doc_topics3=ldamodel3.get_document_topics(corpus2)\n",
    "count2=0\n",
    "count3=0\n",
    "for i in range(len(doc_topics2)):\n",
    "    count2+=len(doc_topics2[i])\n",
    "for i in range(len(doc_topics3)):\n",
    "    count3+=len(doc_topics3[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_answers4(count_model2, count_model3):\n",
    "    with open(\"cooking_LDA_pa_task4.txt\", \"w\") as fout:\n",
    "        fout.write(\" \".join([str(el) for el in [count_model2, count_model3]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200750 1590960"
     ]
    }
   ],
   "source": [
    "save_answers4(count2,count3)\n",
    "!cat cooking_LDA_pa_task4.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таким образом, гиперпараметр __alpha__ влияет на разреженность распределений тем в документах. Аналогично гиперпараметр __eta__ влияет на разреженность распределений слов в темах."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA как способ понижения размерности\n",
    "Иногда, распределения над темами, найденные с помощью LDA, добавляют в матрицу объекты-признаки как дополнительные, семантические, признаки, и это может улучшить качество решения задачи. Для простоты давайте просто обучим классификатор рецептов на кухни на признаках, полученных из LDA, и измерим точность (accuracy).\n",
    "\n",
    "__Задание 5.__ Используйте модель, построенную по сокращенной выборке с alpha по умолчанию (вторую модель). Составьте матрицу $\\Theta = p(t|d)$ вероятностей тем в документах; вы можете использовать тот же метод get_document_topics, а также вектор правильных ответов y (в том же порядке, в котором рецепты идут в переменной recipes). Создайте объект RandomForestClassifier со 100 деревьями, с помощью функции cross_val_score вычислите среднюю accuracy по трем фолдам (перемешивать данные не нужно) и передайте в функцию save_answers5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_topics_csr = gensim.matutils.corpus2csc(doc_topics2)\n",
    "X = all_topics_csr.T.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuisines=list()\n",
    "for i in range(len(recipes)):\n",
    "    cuisines.append(recipes[i]['cuisine'])\n",
    "unique=list(dict.fromkeys(cuisines)) \n",
    "value = np.arange(20)\n",
    "unique_items= {k:v for k, v in zip(unique, value)}\n",
    "#print(unique_items['greek'])\n",
    "y=np.zeros(len(recipes))\n",
    "for i in range(len(y)):\n",
    "    y[i]=(unique_items[recipes[i]['cuisine']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5679094280442902\n"
     ]
    }
   ],
   "source": [
    "clf=RandomForestClassifier(n_estimators=100).fit(X,y)\n",
    "score=cross_val_score(estimator=clf,X=X,y=y,cv=3).mean()\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "##thetta=np.zeros((len(doc_topics2),40))\n",
    "##for i in range(len(doc_topics2)):\n",
    "    ##for j in range(len(doc_topics2[i])):\n",
    "        ##print(i,j)\n",
    "        ##print(doc_topics2[i][j][0])\n",
    "        ##thetta[i][doc_topics2[i][j][0]]=doc_topics2[i][j][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_answers5(accuracy):\n",
    "     with open(\"cooking_LDA_pa_task5.txt\", \"w\") as fout:\n",
    "        fout.write(str(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5679094280442902"
     ]
    }
   ],
   "source": [
    "save_answers5(score)\n",
    "!cat cooking_LDA_pa_task5.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для такого большого количества классов это неплохая точность. Вы можете попроовать обучать RandomForest на исходной матрице частот слов, имеющей значительно большую размерность, и увидеть, что accuracy увеличивается на 10–15%. Таким образом, LDA собрал не всю, но достаточно большую часть информации из выборки, в матрице низкого ранга."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA — вероятностная модель\n",
    "Матричное разложение, использующееся в LDA, интерпретируется как следующий процесс генерации документов.\n",
    "\n",
    "Для документа $d$ длины $n_d$:\n",
    "1. Из априорного распределения Дирихле с параметром alpha сгенерировать распределение над множеством тем: $\\theta_d \\sim Dirichlet(\\alpha)$\n",
    "1. Для каждого слова $w = 1, \\dots, n_d$:\n",
    "    1. Сгенерировать тему из дискретного распределения $t \\sim \\theta_{d}$\n",
    "    1. Сгенерировать слово из дискретного распределения $w \\sim \\phi_{t}$.\n",
    "    \n",
    "Подробнее об этом в [Википедии](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation).\n",
    "\n",
    "В контексте нашей задачи получается, что, используя данный генеративный процесс, можно создавать новые рецепты. Вы можете передать в функцию модель и число ингредиентов и сгенерировать рецепт :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_recipe(model, num_ingredients):\n",
    "    theta = np.random.dirichlet(model.alpha)\n",
    "    for i in range(num_ingredients):\n",
    "        t = np.random.choice(np.arange(model.num_topics), p=theta)\n",
    "        topic = model.show_topic(t, topn=model.num_terms)\n",
    "        topic_distr = [x[1] for x in topic]\n",
    "        terms = [x[0] for x in topic]\n",
    "        w = np.random.choice(terms, p=topic_distr)\n",
    "        print w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Интерпретация построенной модели\n",
    "Вы можете рассмотреть топы ингредиентов каждой темы. Большиснтво тем сами по себе похожи на рецепты; в некоторых собираются продукты одного вида, например, свежие фрукты или разные виды сыра.\n",
    "\n",
    "Попробуем эмпирически соотнести наши темы с национальными кухнями (cuisine). Построим матрицу $A$ размера темы $x$ кухни, ее элементы $a_{tc}$ — суммы $p(t|d)$ по всем документам $d$, которые отнесены к кухне $c$. Нормируем матрицу на частоты рецептов по разным кухням, чтобы избежать дисбаланса между кухнями. Следующая функция получает на вход объект модели, объект корпуса и исходные данные и возвращает нормированную матрицу $A$. Ее удобно визуализировать с помощью seaborn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import seaborn\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_topic_cuisine_matrix(model, corpus, recipes):\n",
    "    # составляем вектор целевых признаков\n",
    "    targets = list(set([recipe[\"cuisine\"] for recipe in recipes]))\n",
    "    # составляем матрицу\n",
    "    tc_matrix = pandas.DataFrame(data=np.zeros((model.num_topics, len(targets))), columns=targets)\n",
    "    for recipe, bow in zip(recipes, corpus):\n",
    "        recipe_topic = model.get_document_topics(bow)\n",
    "        for t, prob in recipe_topic:\n",
    "            tc_matrix[recipe[\"cuisine\"]][t] += prob\n",
    "    # нормируем матрицу\n",
    "    target_sums = pandas.DataFrame(data=np.zeros((1, len(targets))), columns=targets)\n",
    "    for recipe in recipes:\n",
    "        target_sums[recipe[\"cuisine\"]] += 1\n",
    "    return pandas.DataFrame(tc_matrix.values/target_sums.values, columns=tc_matrix.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_matrix(tc_matrix):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    seaborn.heatmap(tc_matrix, square=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Визуализируйте матрицу\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чем темнее квадрат в матрице, тем больше связь этой темы с данной кухней. Мы видим, что у нас есть темы, которые связаны с несколькими кухнями. Такие темы показывают набор ингредиентов, которые популярны в кухнях нескольких народов, то есть указывают на схожесть кухонь этих народов. Некоторые темы распределены по всем кухням равномерно, они показывают наборы продуктов, которые часто используются в кулинарии всех стран. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Жаль, что в датасете нет названий рецептов, иначе темы было бы проще интерпретировать..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Заключение\n",
    "В этом задании вы построили несколько моделей LDA, посмотрели, на что влияют гиперпараметры модели и как можно использовать построенную модель. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
